import os
from huggingface_hub import InferenceClient

token = os.getenv("HF_TOKEN")
if not token:
    raise ValueError("‚ö†Ô∏è Variable d‚Äôenvironnement HF_TOKEN manquante.")

client = InferenceClient(
    model="mistralai/Mistral-7B-Instruct-v0.1",
    token=token
)

def init_chat():
    return [{"role": "system", "content": "Tu es un assistant utile."}]

def ask(prompt, history):
    history.append({"role": "user", "content": prompt})
    full_prompt = "\n".join(f"{m['role']}: {m['content']}" for m in history)
    full_prompt += "\nassistant:"

    try:
        response = client.text_generation(
            full_prompt,
            max_new_tokens=300,
            temperature=0.7,
            stop=["user:", "assistant:"]
        )
        print("üß† R√©ponse brute Mistral :", response)
        answer = response.strip().split("assistant:")[-1].strip()
    except Exception as e:
        answer = f"‚ùå Erreur Mistral: {e}"

    history.append({"role": "assistant", "content": answer})
    return answer
